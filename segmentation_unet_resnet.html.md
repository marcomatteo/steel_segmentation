# Segmentation training Unet


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
# all_slow
```

## Setup

``` python
from fastai.vision.all import *
from fastai.callback.tensorboard import TensorBoardCallback

import random
import cv2
import pandas as pd
import numpy as np

from matplotlib import pyplot as plt

import torch
import albumentations as alb

import segmentation_models_pytorch as smp

from steel_segmentation.utils import get_train_df
from steel_segmentation.transforms import SteelDataBlock, SteelDataLoaders
from steel_segmentation.losses import MultiClassesSoftBCEDiceLoss, LossEnabler
from steel_segmentation.metrics import ModDiceMulti
from steel_segmentation.optimizer import opt_func
```

``` python
def seed_everything(seed=69):
    """
    Seeds `random`, `os.environ["PYTHONHASHSEED"]`,
    `numpy`, `torch.cuda` and `torch.backends`.
    """
    #warnings.filterwarnings("ignore")
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

seed_everything()
```

Training parameters:

``` python
bs = 16
size = (224,512)
epochs = 30
lr = 3e-4
path = Path("../data") # where data dir is
```

## Data loading

``` python
df = get_train_df(path, only_faulty=True, pivot=True)
df.describe(include="all")
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&#10;    .dataframe tbody tr th {
        vertical-align: top;
    }
&#10;    .dataframe thead th {
        text-align: right;
    }
</style>

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: right;">
<th data-quarto-table-cell-role="th">ClassId</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">n</th>
<th data-quarto-table-cell-role="th">ClassIds</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">count</td>
<td>890</td>
<td>245</td>
<td>5078</td>
<td>789</td>
<td>6578.000000</td>
<td>6578</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">unique</td>
<td>890</td>
<td>245</td>
<td>5078</td>
<td>789</td>
<td>NaN</td>
<td>9</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">top</td>
<td>29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882
24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918
27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964
85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104
82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080
57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31
84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57
86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60
88517 60 88772 61 89028 53...</td>
<td>145658 7 145901 20 146144 33 146386 47 146629 60 146872 73 147115 86
147364 93 147620 93 147876 93 148132 93 148388 93 148644 93 148900 93
149156 93 149412 93 149668 46</td>
<td>18661 28 18863 82 19091 110 19347 110 19603 110 19859 110 20115 110
20371 110 20627 110 20883 110 21139 110 21395 110 21651 110 21962 55
293125 251 293381 251 293637 251 293893 251 294149 251 294405 251 294661
251 294917 251 295173 251 295429 251 295685 251 295941 251 296197 251
296453 251 296709 251 296965 251 297221 251 297477 251 297733 251 297989
251 298245 251 298564 188 298945 63</td>
<td>131973 1 132228 4 132483 6 132738 8 132993 11 133248 13 133503 16
133757 19 134012 22 134267 24 134522 26 134777 29 135032 31 135287 34
135542 36 135796 40 136050 43 136304 46 136558 50 136812 54 137066 56
137320 59 137574 61 137828 63 138082 65 138336 68 138590 70 138845 71
139101 71 139356 73 139612 73 139868 73 140123 74 140379 74 140634 75
140890 75 141145 77 141400 78 141654 80 141909 81 142164 82 142418 84
142673 85 142928 86 143182 88 143437 89 143692 90 143946 93 144201 94
144456 95 144710 97 144965 98 145220 99 145474 101 145729 103 145983 105
146237 107 146491 109 146745 112 1469...</td>
<td>NaN</td>
<td>3</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">freq</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>NaN</td>
<td>4691</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">mean</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.064457</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">std</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>0.246820</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">min</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.000000</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">25%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.000000</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">50%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.000000</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">75%</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>1.000000</td>
<td>NaN</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">max</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>3.000000</td>
<td>NaN</td>
</tr>
</tbody>
</table>

</div>

``` python
def get_train_aug(height, width): 
    tfm_list = [
        alb.RandomCrop(height, width, p=1.0),
        alb.OneOf(
          [
           alb.VerticalFlip(p=0.5),
           alb.HorizontalFlip(p=0.5),
          ], p=0.5),
        alb.RandomBrightnessContrast(
            brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),
    ]
    return alb.Compose(tfm_list)

def get_valid_aug(height, width): 
    tfms = [alb.RandomCrop(height, width, p=1.0)]
    return alb.Compose(tfms)

device = "cuda" if torch.cuda.is_available() else "cpu"
train_aug = get_train_aug(*size)
valid_aug = get_valid_aug(*size)
block = SteelDataBlock(path, train_aug=train_aug, valid_aug=valid_aug)
dls = SteelDataLoaders(block, df, bs=bs, device=device)
```

    C:\Users\beanTech\miniconda3\envs\steel_segmentation\lib\site-packages\torch\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\aten\src\ATen\native\BinaryOps.cpp:467.)
      return torch.floor_divide(self, other)

``` python
xb, yb = dls.one_batch()
print(xb.shape, yb.shape)
```

    torch.Size([16, 3, 224, 512]) torch.Size([16, 4, 224, 512])

## Model

``` python
model = smp.Unet(encoder_name="resnet18", encoder_weights="imagenet", classes=4, activation=None)
criterion = BCEWithLogitsLossFlat(pos_weight=torch.tensor([2.0,2.0,1.0,1.5])) # pos_weight because class imbalance
#opt_func = partial(opt_func, torch_opt=torch.optim.Adam) # no need to use pytorch optim
opt_func = RAdam
model_dir = Path("../models")
metrics = [ModDiceMulti(with_logits=True)]
```

``` python
learner = Learner(
    dls = dls,
    model = model,
    loss_func = criterion,
    opt_func = opt_func,
    metrics = metrics,
    model_dir = model_dir,
    cbs = [LossEnabler]
)
```

    C:\Users\beanTech\miniconda3\envs\steel_segmentation\lib\site-packages\fastai\callback\core.py:51: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this
      warn(f"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this")

``` python
learner.summary()
```

    c:\users\beantech\documents\python scripts\steel_segmentation\steel_segmentation\metrics.py:56: RuntimeWarning: Mean of empty slice
      return np.nanmean(binary_dice_scores)

    Unet (Input shape: 16)
    ============================================================================
    Layer (type)         Output Shape         Param #    Trainable 
    ============================================================================
                         16 x 64 x 112 x 256 
    Conv2d                                    9408       True      
    BatchNorm2d                               128        True      
    ReLU                                                           
    MaxPool2d                                                      
    Conv2d                                    36864      True      
    BatchNorm2d                               128        True      
    ReLU                                                           
    Conv2d                                    36864      True      
    BatchNorm2d                               128        True      
    Conv2d                                    36864      True      
    BatchNorm2d                               128        True      
    ReLU                                                           
    Conv2d                                    36864      True      
    BatchNorm2d                               128        True      
    ____________________________________________________________________________
                         16 x 128 x 28 x 64  
    Conv2d                                    73728      True      
    BatchNorm2d                               256        True      
    ReLU                                                           
    Conv2d                                    147456     True      
    BatchNorm2d                               256        True      
    Conv2d                                    8192       True      
    BatchNorm2d                               256        True      
    Conv2d                                    147456     True      
    BatchNorm2d                               256        True      
    ReLU                                                           
    Conv2d                                    147456     True      
    BatchNorm2d                               256        True      
    ____________________________________________________________________________
                         16 x 256 x 14 x 32  
    Conv2d                                    294912     True      
    BatchNorm2d                               512        True      
    ReLU                                                           
    Conv2d                                    589824     True      
    BatchNorm2d                               512        True      
    Conv2d                                    32768      True      
    BatchNorm2d                               512        True      
    Conv2d                                    589824     True      
    BatchNorm2d                               512        True      
    ReLU                                                           
    Conv2d                                    589824     True      
    BatchNorm2d                               512        True      
    ____________________________________________________________________________
                         16 x 512 x 7 x 16   
    Conv2d                                    1179648    True      
    BatchNorm2d                               1024       True      
    ReLU                                                           
    Conv2d                                    2359296    True      
    BatchNorm2d                               1024       True      
    Conv2d                                    131072     True      
    BatchNorm2d                               1024       True      
    Conv2d                                    2359296    True      
    BatchNorm2d                               1024       True      
    ReLU                                                           
    Conv2d                                    2359296    True      
    BatchNorm2d                               1024       True      
    Identity                                                       
    ____________________________________________________________________________
                         16 x 256 x 14 x 32  
    Conv2d                                    1769472    True      
    BatchNorm2d                               512        True      
    ReLU                                                           
    Identity                                                       
    Conv2d                                    589824     True      
    BatchNorm2d                               512        True      
    ReLU                                                           
    Identity                                                       
    ____________________________________________________________________________
                         16 x 128 x 28 x 64  
    Conv2d                                    442368     True      
    BatchNorm2d                               256        True      
    ReLU                                                           
    Identity                                                       
    Conv2d                                    147456     True      
    BatchNorm2d                               256        True      
    ReLU                                                           
    Identity                                                       
    ____________________________________________________________________________
                         16 x 64 x 56 x 128  
    Conv2d                                    110592     True      
    BatchNorm2d                               128        True      
    ReLU                                                           
    Identity                                                       
    Conv2d                                    36864      True      
    BatchNorm2d                               128        True      
    ReLU                                                           
    Identity                                                       
    ____________________________________________________________________________
                         16 x 32 x 112 x 256 
    Conv2d                                    36864      True      
    BatchNorm2d                               64         True      
    ReLU                                                           
    Identity                                                       
    Conv2d                                    9216       True      
    BatchNorm2d                               64         True      
    ReLU                                                           
    Identity                                                       
    ____________________________________________________________________________
                         16 x 16 x 224 x 512 
    Conv2d                                    4608       True      
    BatchNorm2d                               32         True      
    ReLU                                                           
    Conv2d                                    2304       True      
    BatchNorm2d                               32         True      
    ReLU                                                           
    Identity                                                       
    ____________________________________________________________________________
                         16 x 4 x 224 x 512  
    Conv2d                                    580        True      
    Identity                                                       
    Identity                                                       
    ____________________________________________________________________________

    Total params: 14,328,644
    Total trainable params: 14,328,644
    Total non-trainable params: 0

    Optimizer used: <function RAdam at 0x000001E9DC086DC0>
    Loss function: FlattenedLoss of BCEWithLogitsLoss()

    Callbacks:
      - TrainEvalCallback
      - LossEnabler
      - Recorder
      - ProgressCallback

``` python
learner.show_training_loop()
```

    Start Fit
       - before_fit     : [TrainEvalCallback, Recorder, ProgressCallback]
      Start Epoch Loop
         - before_epoch   : [Recorder, ProgressCallback]
        Start Train
           - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]
          Start Batch Loop
             - before_batch   : []
             - after_pred     : [LossEnabler]
             - after_loss     : []
             - before_backward: []
             - before_step    : []
             - after_step     : []
             - after_cancel_batch: []
             - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]
          End Batch Loop
        End Train
         - after_cancel_train: [Recorder]
         - after_train    : [Recorder, ProgressCallback]
        Start Valid
           - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]
          Start Batch Loop
             - **CBs same as train batch**: []
          End Batch Loop
        End Valid
         - after_cancel_validate: [Recorder]
         - after_validate : [Recorder, ProgressCallback]
      End Epoch Loop
       - after_cancel_epoch: []
       - after_epoch    : [Recorder]
    End Fit
     - after_cancel_fit: []
     - after_fit      : [ProgressCallback]

Logging with the TensorBoardCallback:

``` python
# logging info
log_dir = Path("../logs") / f"unet_resnet_bce_epochs{epochs}_lr{lr}"
log_dir
```

    Path('../logs/unet_resnet_bce_epochs30_lr0.0003')

``` python
train_cbs = [
    TensorBoardCallback(log_dir=log_dir, log_preds=True, trace_model=True, projector=False),
    GradientAccumulation(n_acc=24),
    SaveModelCallback(monitor="valid_loss", fname=log_dir.name, with_opt=True),
]
```

``` python
learner.fit(epochs, lr=lr, cbs=train_cbs)
```

<table class="dataframe" data-quarto-postprocess="true" data-border="1">
<thead>
<tr style="text-align: left;">
<th data-quarto-table-cell-role="th">epoch</th>
<th data-quarto-table-cell-role="th">train_loss</th>
<th data-quarto-table-cell-role="th">valid_loss</th>
<th data-quarto-table-cell-role="th">mod_dice_multi</th>
<th data-quarto-table-cell-role="th">time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.051801</td>
<td>0.040153</td>
<td>0.527716</td>
<td>04:04</td>
</tr>
</tbody>
</table>
