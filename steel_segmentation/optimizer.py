# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_optimizer.ipynb.

# %% auto 0
__all__ = ['params', 'convert_params', 'smp_splitter', 'opt_func']

# %% ../nbs/03_optimizer.ipynb 3
import torch.nn as nn
from fastcore.foundation import L
from fastai.optimizer import OptimWrapper

# %% ../nbs/03_optimizer.ipynb 4
def params(m):
    "Return all parameters of `m` (Pytorch model)."
    return [p for p in m.parameters()]

# %% ../nbs/03_optimizer.ipynb 5
def convert_params(o:list) -> list:
    """
    Converts `o` into Pytorch-compatable param groups
    `o` should be a set of layer-groups that should be split in the optimizer
    Example:
    ```python
    def splitter(m): return convert_params([[m.a], [m.b]])
    ```
    Where `m` is a model defined as:
    ```python
    class RegModel(Module):
      def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))
      def forward(self, x): return x*self.a + self.b
    ```
    """
    if not isinstance(o[0], dict):
        splitter = []
        for group in o:
            if not isinstance(group[0], nn.parameter.Parameter):
                group = L(group).map(params)[0]
            splitter.append({'params':group})
        return splitter
    return o

# %% ../nbs/03_optimizer.ipynb 7
def smp_splitter(model):
    return convert_params([[model.encoder],[model.decoder],[model.segmentation_head]])

# %% ../nbs/03_optimizer.ipynb 8
def opt_func(params, torch_opt, *args, **kwargs):
    """Pytorch Optimizer for fastai Learner."""
    return OptimWrapper(params, torch_opt, *args, **kwargs)
