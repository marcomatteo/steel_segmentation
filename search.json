[
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "The fastai data block API cannot give us a tuple (input,target) data for our Deep Learning model. Thatâ€™s why we need to create custom Transforms to fix this problem and leverage anyway the high level framework.\npath = Path(\"../data\")\nassert path.is_dir()\ntrain_pivot = get_train_df(path, only_faulty=True, pivot=True)\nprint(\"Train pivot shape: \", train_pivot.shape)\ntrain_pivot.head(n=3)\n\nTrain pivot shape:  (6666, 6)\n\n\n\n\n\n\n\n\nClassId\n1\n2\n3\n4\nn\nClassIds\n\n\nImageId\n\n\n\n\n\n\n\n\n\n\n0002cc93b.jpg\n29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53...\nNaN\nNaN\nNaN\n1\n1\n\n\n0007a71bf.jpg\nNaN\nNaN\n18661 28 18863 82 19091 110 19347 110 19603 110 19859 110 20115 110 20371 110 20627 110 20883 110 21139 110 21395 110 21651 110 21962 55 293125 251 293381 251 293637 251 293893 251 294149 251 294405 251 294661 251 294917 251 295173 251 295429 251 295685 251 295941 251 296197 251 296453 251 296709 251 296965 251 297221 251 297477 251 297733 251 297989 251 298245 251 298564 188 298945 63\nNaN\n1\n3\n\n\n000a4bcdd.jpg\n37607 3 37858 8 38108 14 38359 20 38610 25 38863 28 39119 28 39375 29 39631 29 39887 29 40143 29 40399 29 40655 30 40911 30 41167 30 41423 30 41679 31 41935 31 42191 31 42447 31 42703 31 42960 31 43216 31 43472 31 43728 31 43984 31 44240 32 44496 32 44752 32 45008 32 45264 33 45520 33 45776 33 46032 33 46288 33 46544 34 46803 31 47065 25 47327 19 47588 15 47850 9 48112 3 62667 12 62923 23 63179 23 63348 3 63435 23 63604 7 63691 23 63860 11 63947 23 64116 15 64203 23 64372 19 64459 23 64628 24 64715 23 64884 28 64971 23 65139 33 65227 23 65395 37 65483 23 65651 41 65740 22 65907 45 65996 22...\nNaN\nNaN\nNaN\n1\n1\ntrain_example = train_pivot.sample().iloc[0]\ntrain_example.name\n\n'fdab86eb7.jpg'"
  },
  {
    "objectID": "transforms.html#the-data-pipeline-steelmask-block",
    "href": "transforms.html#the-data-pipeline-steelmask-block",
    "title": "Transforms",
    "section": "The Data pipeline: SteelMask Block",
    "text": "The Data pipeline: SteelMask Block\n\nGet_x transform\n\nsource\n\n\nReadImagePathFromIndex\n\n ReadImagePathFromIndex (pref)\n\nRead image name from train_pivot and returns the image path\nWith the ReadImagePathFromIndex transform we can get the first information of the training example, the image path for train_example from train_pivot. In setup we need to specify the prefix to be added to the ImageId of the training image.\n\nx_tfm = ReadImagePathFromIndex(pref=(path/\"train_images\"))\n\n\nx = x_tfm(train_example)\ntest_eq(x, str(path/\"train_images\"/train_example.name))\n\n\n\nGet_y transform\n\nsource\n\n\nReadRLEs\n\n ReadRLEs (cols=[1, 2, 3, 4])\n\nRead RLEs from train_pivot and return a list or RLEs.\nWith the ReadRLEs transform we can get our labels, the list of RLEs (one for each ClassIds). In the setup we need to specify the column names of the ClassId. In input we pass the train_example and we get a list of strings as output.\n\ncols = [1,2,3,4]\ny_tfm = ReadRLEs(cols=cols)\nrles = y_tfm(train_example)\ntest_eq(len(rles), 4)\ntest_eq(rles, [train_example[i] if not train_example[i] is np.nan else '' for i in cols])\n\n\n\nGet Mask from RLEs\n\nsource\n\n\nMakeMask\n\n MakeMask (flatten=True)\n\nRead RLEs list and return a np.array of the mask\nThe MakeMask transform needs a list of RLEs and returns a mask with (256, 1600) shape if flatten is True (default). If flatten is False returns a (256, 1600, 4) array.\n\nmask_tfm = MakeMask(flatten=False)\nmask = mask_tfm(rles)\ntest_eq(mask.shape, (256,1600,4))\n# Default transform with flatten mask for PILMask.create\nflatten_mask_tfm = MakeMask()\nflatten_mask = flatten_mask_tfm(rles)\ntest_eq(flatten_mask.shape, (256,1600))\n\n\nplt.figure(figsize=(15,5))\nplt.xticks([])\nplt.yticks([])\nplt.imshow(flatten_mask);\n\n\n\n\n\n\n\n\n\nrle = mask_tfm.decode(mask)\ntest_eq(rle, rles)\nflatten_rle = flatten_mask_tfm.decode(flatten_mask)\ntest_eq(flatten_rle, rles)\n\nFinally, a Datasets object can be built from the two Pipelines created with the previous transforms.\n\nx_tfms = Pipeline([x_tfm, PILImage.create])\ny_tfms = Pipeline([y_tfm, flatten_mask_tfm, PILMask.create])\ndsets = Datasets(train_pivot, [x_tfms, y_tfms])\nelem = dsets.train[1]\nimage, mask = elem\ntype(elem), image, mask\n\n(tuple, PILImage mode=RGB size=1600x256, PILMask mode=F size=1600x256)\n\n\n\n_,axs = plt.subplots(1,3, figsize=(20, 5))\nimage.show(ctx=axs[0], title='image')\n\nmask.show(alpha=1, ctx=axs[1], vmin=1, vmax=30, title='mask')\n\nimage.show(ctx=axs[2], title='superimposed')\nmask.show(ctx=axs[2], vmin=1, vmax=30);\n\n\n\n\n\n\n\n\n\n\n4-channel Mask\n\nsource\n\n\nChannelMask\n\n ChannelMask (enc=None, dec=None, split_idx=None, order=None)\n\nTransform (x,y) tensor masks from [w, h] to [channels, w, h]\nThe ChannelMask transform changes the shape of the mask from a flatten (256, 1600) to (4, 256, 1600).\n\ntens = ToTensor()\ntimg, tmask = tens(elem)\ntimg.shape, tmask.shape, tmask.dim()\n\n(torch.Size([3, 256, 1600]), torch.Size([256, 1600]), 2)\n\n\n\ntfm = ChannelMask()\nch_mask = tfm(tmask)\nch_mask.shape, ch_mask.dim()\n\n(torch.Size([4, 256, 1600]), 3)\n\n\n\ndecoded_mask = tfm.decodes(ch_mask)\ndecoded_mask.shape\n\ntorch.Size([256, 1600])\n\n\n\ntest_close(decoded_mask, tmask)\n\n\nshow_images((decoded_mask,tmask), figsize=(15,5));\n\n\n\n\n\n\n\n\nIt works with batches:\n\nbs_tmask = tmask.unsqueeze(0).expand(6, -1, -1)\ntfm = ChannelMask()\nbs_ch_mask = tfm(bs_tmask)\nbs_ch_mask.shape, bs_ch_mask.dim()\n\n(torch.Size([6, 4, 256, 1600]), 4)\n\n\n\ndecoded_bs_mask = tfm.decodes(bs_ch_mask)\ndecoded_bs_mask.shape\n\ntorch.Size([6, 256, 1600])\n\n\n\nfor ch, tmp_mask in enumerate(bs_ch_mask):\n    test_close(decoded_bs_mask[ch, ...], bs_tmask[ch, ...])\n\n\n\nAlbumentation transforms\n\nimg, mask = elem\nimg, mask = np.array(img), np.array(mask)\nimg.shape, mask.shape\n\n((256, 1600, 3), (256, 1600))\n\n\nSome augmentations from the albumentation library:\n\nimport cv2\ndef show_aug(aug, img, mask):\n    aug_elem = aug(image=img, mask=mask)\n    aug_crop_img = aug_elem[\"image\"]\n    aug_crop_mask = aug_elem[\"mask\"]\n    print(aug_crop_img.shape, aug_crop_mask.shape)\n    print(f\"Unique elems in mask: {np.unique(aug_crop_mask)}\")\n    show_images((aug_crop_img, aug_crop_mask), figsize=(10,20))\n    return aug_crop_img, aug_crop_mask\n\n\naug = alb.CropNonEmptyMaskIfExists(256, 400, p=1., ignore_values=[0])\naug_crop_img, aug_crop_mask = show_aug(aug, img, mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\naug = alb.VerticalFlip(p=1.)\naug_img, aug_mask = show_aug(aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\naug = alb.HorizontalFlip(p=1.)\naug_img, aug_mask = show_aug(aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\naug = alb.ElasticTransform(p=1., alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)\naug_img, aug_mask = show_aug(aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\naug = alb.GridDistortion(p=1.)\naug_img, aug_mask = show_aug(aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\naug = alb.OpticalDistortion(distort_limit=0.5, shift_limit=0.05, p=1., border_mode=cv2.BORDER_REPLICATE)\naug_img, aug_mask = show_aug(aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\nAll is wrapped up in get_train_aug and get_valid_aug for training and validation augmentations. Then AlbumentationTransform is a mixed Transform for the DataBlock.\n\nsource\n\n\nAlbumentationsTransform\n\n AlbumentationsTransform (train_aug, valid_aug)\n\nA transform handler for multiple Albumentation transforms\n\nsource\n\n\nget_valid_aug\n\n get_valid_aug (height, width)\n\n\nsource\n\n\nget_train_aug\n\n get_train_aug (height, width)\n\n\ntrain_aug = get_train_aug(256, 400)\naug_img, aug_mask = show_aug(train_aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\nvalid_aug = get_valid_aug(256, 400)\naug_img, aug_mask = show_aug(valid_aug, aug_crop_img, aug_crop_mask)\n\n(256, 400, 3) (256, 400)\nUnique elems in mask: [0. 3.]\n\n\n\n\n\n\n\n\n\n\nalb_tfm = AlbumentationsTransform(train_aug, valid_aug)\nalb_aug_elem = alb_tfm(elem, split_idx=0)\nshow_images(alb_aug_elem, figsize=(15, 10))\n\n\n\n\n\n\n\n\n\n\nSteelMaskBlock\n\nsource\n\n\nSteelMaskBlock\n\n SteelMaskBlock (train_aug, valid_aug)"
  },
  {
    "objectID": "transforms.html#steeldatablock",
    "href": "transforms.html#steeldatablock",
    "title": "Transforms",
    "section": "SteelDatablock",
    "text": "SteelDatablock\n\nsource\n\nSteelDataBlock\n\n SteelDataBlock (path, splitter=None, train_aug=None, valid_aug=None,\n                 *args, **kwargs)\n\nGet the DataBlock for Severstal Dataset.\n\nsplitter = TrainTestSplitter(0.15)\nblock = SteelDataBlock(path, splitter)\ndls = block.dataloaders(\n    source=train_pivot, \n    bs=16, \n    num_workers=0\n)\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\n\n\n(torch.Size([16, 3, 224, 1568]), torch.Size([16, 4, 224, 1568]))"
  },
  {
    "objectID": "transforms.html#k-folds-strategy",
    "href": "transforms.html#k-folds-strategy",
    "title": "Transforms",
    "section": "K-Folds strategy",
    "text": "K-Folds strategy\nThe scikit-learn StratifiedKFold class for this dataset.\n\nnsplits = 2\n# df = train_pivot[[1,2,3,4]].stack().to_frame().reset_index()\ndf = train_pivot.reset_index()\nX = df[\"ImageId\"].to_numpy()\ny = df[\"ClassIds\"].to_numpy()\nX.shape, y.shape\n\n((6578,), (6578,))\n\n\n\nskf = StratifiedKFold(n_splits=nsplits, shuffle=True)\ndsets = {i: _ for i in range(nsplits)}\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    print(\n        f\"{i}-fold:\",\n        f\"Train: #{len(train_index)}, e.g. {train_index[:5]}\", \n        f\"Valid: #{len(valid_index)}, e.g. {valid_index[:5]}\", \n        sep='\\n', end='\\n\\n')\n\n0-fold:\nTrain: #3289, e.g. [ 1  6 12 13 15]\nValid: #3289, e.g. [0 2 3 4 5]\n\n1-fold:\nTrain: #3289, e.g. [0 2 3 4 5]\nValid: #3289, e.g. [ 1  6 12 13 15]\n\n\n\n\nsource\n\nget_kfold_splits\n\n get_kfold_splits (df_pivot, nsplits=2)\n\n\nsplits = get_kfold_splits(train_pivot, nsplits=2)\n\n0-fold:\nTrain: #3289, e.g. [ 0  2  5  6 12]\nValid: #3289, e.g. [1 3 4 7 8]\n\n1-fold:\nTrain: #3289, e.g. [1 3 4 7 8]\nValid: #3289, e.g. [ 0  2  5  6 12]\n\n\n\n\nsource\n\n\nKFoldSplitter\n\n KFoldSplitter (splits, idx)\n\n\nsplits\n\n[[(#3289) [0,2,5,6,12,17,18,19,23,26...],\n  (#3289) [1,3,4,7,8,9,10,11,13,14...]],\n [(#3289) [1,3,4,7,8,9,10,11,13,14...],\n  (#3289) [0,2,5,6,12,17,18,19,23,26...]]]"
  },
  {
    "objectID": "transforms.html#steeldataloaders",
    "href": "transforms.html#steeldataloaders",
    "title": "Transforms",
    "section": "SteelDataLoaders",
    "text": "SteelDataLoaders\n\nsource\n\nSteelDataLoaders\n\n SteelDataLoaders (block, source, bs, *args, **kwargs)\n\nGet the DataLoaders for Severstal Dataset.\n\ndls = SteelDataLoaders(block, train_pivot, bs=16, size=(256, 400))\nxb, yb = dls.one_batch()\nxb.shape, yb.shape\n\n(torch.Size([16, 3, 224, 1568]), torch.Size([16, 4, 224, 1568]))\n\n\n\nfor idx, split in enumerate(splits):\n    block = SteelDataBlock(path, splitter=KFoldSplitter(splits, idx))\n    dls = SteelDataLoaders(block, train_pivot, bs=16, size=(256, 400))\n    print(f\"{i} - Train items:\\n{dls.train.items.ClassIds.value_counts()}\")\n    print(f\"{i} - Valid items:\\n{dls.valid.items.ClassIds.value_counts()}\")\n\n1 - Train items:\n3        2346\n1         381\n4         254\n3 4       140\n2          97\n1 3        45\n1 2        18\n2 3         7\n1 2 3       1\nName: ClassIds, dtype: int64\n1 - Valid items:\n3        2345\n1         382\n4         254\n3 4       141\n2          97\n1 3        45\n1 2        17\n2 3         7\n1 2 3       1\nName: ClassIds, dtype: int64\n1 - Train items:\n3        2345\n1         382\n4         254\n3 4       141\n2          97\n1 3        45\n1 2        17\n2 3         7\n1 2 3       1\nName: ClassIds, dtype: int64\n1 - Valid items:\n3        2346\n1         381\n4         254\n3 4       140\n2          97\n1 3        45\n1 2        18\n2 3         7\n1 2 3       1\nName: ClassIds, dtype: int64"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "",
    "text": "This repository wants to explore different solutions for the Severstal competition hosted by Kaggle. Kaggle is a platform that provides various datasets from the real world machine learning problems and engages a large community of people. Severstal is a Russian company operating in the steel and mining industry. It creates a vast industrial data lake and in the 2019 looked to machine learning to improve automation, increase efficiency, and maintain high quality in their production.\nThe goal is to detect steel defects with segmentation models. The solutions are based on Pytorch with FastAI as high level deep learning framework.\nIn this repository you will find some Jupyter Notebooks used to build the steel_segmentation library with nbdev and the training notebooks.\nIn the steel_deployment repository you can find a Binder/Voila web app for the deployment of the models built with this library (still updating)."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "Install",
    "text": "Install\nTo install this package, clone and install the repository and install via:\npip install git+https://github.com/marcomatteo/steel_segmentation.git\n\nEditable install\nTo install and edit this package:\nclone git+https://github.com/marcomatteo/steel_segmentation.git\npip install -e steel_segmentation\nThe library is based on nbdev, a powerful tool that builds a python package from Juptyer Notebooks.\npip install nbdev\nTo create the library, the documentation and tests use these commands:\nnbdev_clean_nbs\nnbdev_build_lib\nnbdev_test_nbs\nnbdev_build_docs\nThis enviroment works on MacOS and Linux. In Windows the WLS with Ubuntu 20.04 is raccomended.\nTraining only in Windows needs one package more to solve ipykernel issues:\nconda install pywin32"
  },
  {
    "objectID": "index.html#download-the-dataset",
    "href": "index.html#download-the-dataset",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "Download the dataset",
    "text": "Download the dataset\nTo download the Kaggle competition data you will need an account (if this is the first time with the API follow this link) to generate the credentials, download and copy the kaggle.json into the repository directory.\n\n!mkdir ~/.kaggle\n!cp ../kaggle.json ~/.kaggle/kaggle.json\n!chmod 600 ~/.kaggle/kaggle.json\n\nNow youâ€™re authenticated with the Kaggle API (youâ€™ll need kaggle so pip install kaggle first), download and unzip the data:\n\n!kaggle competitions download -c severstal-steel-defect-detection -p {path}\n!mkdir data\n!unzip -q -n {path}/severstal-steel-defect-detection.zip -d {path}"
  },
  {
    "objectID": "index.html#library-notebooks",
    "href": "index.html#library-notebooks",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "Library notebooks",
    "text": "Library notebooks\nAll of the experiments are based on Jupyter Notebooks and in the nbs folder there are all the notebooks used to build the steel_segmentation library (still updating):\n\nExplorating Data Analysis: data analysis, plots and utility functions.\nTransforms: leveraging Middle-level API of fastai for custom data loading pipeline.\nOptimizer utility functions\nLoss functions\nMetrics"
  },
  {
    "objectID": "index.html#training",
    "href": "index.html#training",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "Training",
    "text": "Training\nTraining script in scripts folder:\n\nsegmentation_train.py: training segmentation models from qubvel repository.\ncreate_submission.py : create a kaggle submission from a segmentation model trained and save the csv in data/submissions/."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Kaggle Severstal Steel Defect Detection",
    "section": "Results",
    "text": "Results\n\n\n\nModels\nPublic score\nPrivate score\nPercentile Private LB\n\n\n\n\nPytorch UNET-ResNet18\n0.87530\n0.85364\n85Â°\n\n\nPytorch UNET-ResNet34\n0.88591\n0.88572\n46Â°\n\n\nFastAI UNET-ResNet34\n0.88648\n0.88830\n23Â°\n\n\nPytorch FPN-ResNet34\n0.89054\n0.88911\n19Â°\n\n\nEnsemble UNET-ResNet34_FPN-ResNet34\n0.89184\n0.89262\n16Â°"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "from fastai.vision.all import *\nimport numpy as np\nfrom torch.nn.modules.loss import _Loss\nimport segmentation_models_pytorch as smp\nfrom steel_segmentation.utils import get_train_df\nfrom steel_segmentation.transforms import SteelDataBlock, SteelDataLoaders\npath = Path(\"../data\")\ntrain_pivot = get_train_df(path=path, pivot=True)\nblock = SteelDataBlock(path)\ndls = SteelDataLoaders(block, train_pivot, bs=8)\nxb, yb = dls.one_batch()\nprint(xb.shape, xb.device)\nprint(yb.shape, yb.device)\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\n\n\ntorch.Size([8, 3, 224, 1568]) cuda:0\ntorch.Size([8, 4, 224, 1568]) cpu\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\nmodel = smp.Unet(\"resnet18\", classes=4).to(device)\n\nlogits = model(xb)\nprobs = torch.sigmoid(logits)\npreds = ( probs &gt; 0.5).float()"
  },
  {
    "objectID": "metrics.html#kaggle-dice-metric",
    "href": "metrics.html#kaggle-dice-metric",
    "title": "Metrics",
    "section": "Kaggle Dice metric",
    "text": "Kaggle Dice metric\nThe competition evaluation metric is defined as:\n\nThis competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n\\[\nJ(A,B) = \\frac{2 * |A \\cap B|}{|A| \\cup |B|}\n\\]\n\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each &lt;ImageId, ClassId&gt; pair in the test set.\n\nIn this section there are all the metric that can be used to evaluate the performances of the segmentation models trained.\nSimulated training with compute_val and a test Learner with TstLearner.\n\n#For testing: a fake learner and a metric that isn't an average\n@delegates()\nclass TstLearner(Learner):\n    def __init__(self,dls=None,model=None,**kwargs): \n        self.pred,self.xb,self.yb = None,None,None\n        self.loss_func=BCEWithLogitsLossFlat()\n        \n#Go through a fake cycle with various batch sizes and computes the value of met\ndef compute_val(met, pred, y):\n    met.reset()\n    vals = [0,6,15,20]\n    learn = TstLearner()\n    for i in range(3):\n        learn.pred = pred[vals[i]:vals[i+1]]\n        learn.yb = ( y[vals[i]:vals[i+1]], )\n        met.accumulate(learn)\n    return met.value"
  },
  {
    "objectID": "metrics.html#multiclass-dice",
    "href": "metrics.html#multiclass-dice",
    "title": "Metrics",
    "section": "Multiclass Dice",
    "text": "Multiclass Dice\nThe fastai library comes with a dice metric for multiple channel masks. As a segmentation metric in this frameworks, it expects a flatten mask for targets.\n\nmultidice_obj = DiceMulti()\n\n\ncompute_val(multidice_obj, pred=preds.detach().cpu(), y=yb.argmax(1))\n\n0.1798790120410166\n\n\nHere we slightly change the DiceMulti for a 4-channel mask as targets.\n\nsource\n\nModDiceMulti\n\n ModDiceMulti (axis=1, with_logits=False)\n\nAveraged Dice metric (Macro F1) for multiclass target in segmentation\n\ndice_obj = ModDiceMulti(with_logits=True)\ncompute_val(dice_obj, pred=logits.detach().cpu(), y=yb)\n\n0.2130325182791189\n\n\n\ndice_obj = ModDiceMulti()\ncompute_val(dice_obj, pred=preds.detach().cpu(), y=yb)\n\n0.2130325182791189"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploration Data Analysis",
    "section": "",
    "text": "import fastai\nprint(fastai.__version__)\n\n2.8.4\nsource"
  },
  {
    "objectID": "eda.html#data-structure",
    "href": "eda.html#data-structure",
    "title": "Exploration Data Analysis",
    "section": "Data structure",
    "text": "Data structure\n\nsource\n\nprint_competition_data\n\n print_competition_data (p:pathlib.Path)\n\n\npath = Path(\"../data/\")\n\nThe path variable is a os.pathlib.Path object that points to the competition data.\nTo print all the files this directory use the print_competition_data function.\n\nprint_competition_data(path)\n\n..\\data\\hard_negatives_patterns.txt\n..\\data\\predictions\n..\\data\\sample_submission.csv\n..\\data\\submissions\n..\\data\\test_images\n..\\data\\train.csv\n..\\data\\train_images\n\n\nThe competition files in detail:\n\ntrain_images/ - folder of training images (12.5k images)\ntest_images/ - folder of test images to segment and classify (5506 images)\ntrain.csv - training annotations which provide segments for defects (ClassId = [1, 2, 3, 4])\nsample_submission.csv - a sample submission file in the correct format; note, each ImageId 4 rows, one for each of the 4 defect classes\n\n\ntrain_path = path/\"train_images\"\ntest_path = path/\"test_images\"\ntrain_pfiles = get_image_files(train_path)\ntest_pfiles = get_image_files(test_path)"
  },
  {
    "objectID": "eda.html#training-data",
    "href": "eda.html#training-data",
    "title": "Exploration Data Analysis",
    "section": "Training data",
    "text": "Training data\nThe training data includes:\n\nfaulty images: images that have at least one defect\nhard negative images: images with no defects\n\n\ntrain_pfiles\n\n(#12568) [Path('../data/train_images/0002cc93b.jpg'),Path('../data/train_images/00031f466.jpg'),Path('../data/train_images/000418bfc.jpg'),Path('../data/train_images/000789191.jpg'),Path('../data/train_images/0007a71bf.jpg'),Path('../data/train_images/000a4bcdd.jpg'),Path('../data/train_images/000f6bf48.jpg'),Path('../data/train_images/0014fce06.jpg'),Path('../data/train_images/001982b08.jpg'),Path('../data/train_images/001d1b355.jpg')...]\n\n\nThe get_train_df function returns the DataFrame from the train.csv file, only faulty image names if only_faulty, with the training images metadata: - ImageId: image name\n\nClassId: the class type\nEncodedPixels: the encoded pixels follows a run-length encoding rule, a sequence of pair values that contains a start position and a run length with the space as the delimiter. E.g. 1 3 10 5 means pixels (1,2,3) and (10,11,12,13,14).\n\nEach Image may have no defects, a single defect, or multiple defects.\n\nsource\n\nget_train_df\n\n get_train_df (path, only_faulty=False, pivot=False, hard_negatives=False)\n\nGet training DataFrame with all the images in data/train_images. Returns only the faulty images if only_faulty.\n\nsource\n\n\nget_train_pivot\n\n get_train_pivot (df)\n\nSummarize the training csv with ClassId as columns and values EncodedPixels\n\n\nHard negatives with patterns\n\nhard_neg_patterns = pd.read_csv(\n    path/\"hard_negatives_patterns.txt\", header=None, names=[\"ImageId\"])\n\n\nhard_neg_patterns.head()\n\n\n\n\n\n\n\n\nImageId\n\n\n\n\n0\n000789191.jpg\n\n\n1\n00d7ae946.jpg\n\n\n2\n01b237ab8.jpg\n\n\n3\n01d590c5f.jpg\n\n\n4\n01e501f99.jpg\n\n\n\n\n\n\n\nIn hard_neg_patterns there are the ImageIds of training images with some patterns (from this kernel).\n\ntrain = get_train_df(path, only_faulty=False)\nassert isinstance(train, pd.DataFrame)\ntrain_faulty = train.loc[train[\"status\"]==\"faulty\"]\nassert not train.ImageId_ClassId.duplicated().any(), \"Found ImageId_ClassId duplicates\"\n\n\ntrain.describe(include='all')[:4]\n\n\n\n\n\n\n\n\nImageId\nClassId\nEncodedPixels\nstatus\nImageId_ClassId\n\n\n\n\ncount\n12997\n12997.0\n12997.0\n12997\n12997\n\n\nunique\n12568\nNaN\n7096.0\n2\n12997\n\n\ntop\nef24da2ba.jpg\nNaN\n-1.0\nfaulty\n0002cc93b.jpg_1\n\n\nfreq\n3\nNaN\n5902.0\n7095\n1\n\n\n\n\n\n\n\n\nprint(train.shape)\ntrain.head(2)\n\n(12997, 5)\n\n\n\n\n\n\n\n\n\nImageId\nClassId\nEncodedPixels\nstatus\nImageId_ClassId\n\n\n\n\n0\n0002cc93b.jpg\n1\n29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53...\nfaulty\n0002cc93b.jpg_1\n\n\n1\n00031f466.jpg\n0\n-1\nno_faulty\n00031f466.jpg_0\n\n\n\n\n\n\n\n\nmissing_imgs = train[\"ImageId\"].map(lambda x: not ((path/\"train_images\"/str(x)).is_file()))\nprint(missing_imgs.sum())\n\n0\n\n\n\nprint(train_faulty.shape)\ntrain_faulty.head(2)\n\n(7095, 5)\n\n\n\n\n\n\n\n\n\nImageId\nClassId\nEncodedPixels\nstatus\nImageId_ClassId\n\n\n\n\n0\n0002cc93b.jpg\n1\n29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53...\nfaulty\n0002cc93b.jpg_1\n\n\n4\n0007a71bf.jpg\n3\n18661 28 18863 82 19091 110 19347 110 19603 110 19859 110 20115 110 20371 110 20627 110 20883 110 21139 110 21395 110 21651 110 21962 55 293125 251 293381 251 293637 251 293893 251 294149 251 294405 251 294661 251 294917 251 295173 251 295429 251 295685 251 295941 251 296197 251 296453 251 296709 251 296965 251 297221 251 297477 251 297733 251 297989 251 298245 251 298564 188 298945 63\nfaulty\n0007a71bf.jpg_3\n\n\n\n\n\n\n\n\nsource\n\n\ncount_pct\n\n count_pct (df, column='ClassId')\n\nReturns a pandas.DataFrame with count and frequencies stats for column.\nThe ClassId column values from train are:\n\nclass_count = count_pct(train)\nclass_count\n\n\n\n\n\n\n\n\nnum\nfreq\n\n\nClassId\n\n\n\n\n\n\n0\n5902\n0.454105\n\n\n1\n897\n0.069016\n\n\n2\n247\n0.019004\n\n\n3\n5150\n0.396245\n\n\n4\n801\n0.061630\n\n\n\n\n\n\n\n\nclass_count[\"num\"].plot.bar(title=\"Defects by ClassId count\");\n\n\n\n\n\n\n\n\nImages have at least one defect and thereâ€™s a small number of images with two or three defects.\n\ncounts = train_faulty[\"ImageId\"].value_counts()\n\nhist_counts, _ = np.histogram(counts.values, bins=3)\nnums = ['1', '2', '3']\nplt.bar(x=nums, height=hist_counts)\nplt.title(\"Num of defects per images\")\nplt.show()\n\n{i+1: c for i, c in enumerate(hist_counts)}\n\n\n\n\n\n\n\n\n{1: 6239, 2: 425, 3: 2}\n\n\n\nunique_imgs = train_faulty.describe(include='all')[\"ImageId\"].T[:2]\nunique_imgs\n\ncount     7095\nunique    6666\nName: ImageId, dtype: object\n\n\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 6.5))\nax.set_title(\"Count imgs\", pad=30, fontdict={'fontsize': 14})\nax.xaxis.tick_top()  # Display x-axis ticks on top\n\n(\n    train[['ImageId', 'status']]\n    .drop_duplicates()\n    .status.value_counts().iloc[:-1]\n    .plot.barh(table=True, ax=ax)\n)\nplt.show()"
  },
  {
    "objectID": "eda.html#train-transforms",
    "href": "eda.html#train-transforms",
    "title": "Exploration Data Analysis",
    "section": "Train transforms",
    "text": "Train transforms\nLoading the images for models requires some transformations to the DataFrames.\n\nPivot RLE encodings\nThe get_train_pivot is the pivoted version of df. All the images are in the index, for each image the ClassId encoding values are in the columns.\n\ntrain_pivot = get_train_df(path, pivot=True)\n\n\ntrain_pivot.head(2)\n\n\n\n\n\n\n\nClassId\n0\n1\n2\n3\n4\nn\nClassIds\n\n\nImageId\n\n\n\n\n\n\n\n\n\n\n\n0002cc93b.jpg\nNaN\n29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53...\nNaN\nNaN\nNaN\n1\n1\n\n\n00031f466.jpg\n-1\nNaN\nNaN\nNaN\nNaN\n1\n0\n\n\n\n\n\n\n\n\ncount_pct(train_pivot, column='n')\n\n\n\n\n\n\n\n\nnum\nfreq\n\n\nn\n\n\n\n\n\n\n1\n12141\n0.966025\n\n\n2\n425\n0.033816\n\n\n3\n2\n0.000159\n\n\n\n\n\n\n\n\ncount_pct(train_pivot, column='ClassIds').sort_values(\"freq\")\n\n\n\n\n\n\n\n\nnum\nfreq\n\n\nClassIds\n\n\n\n\n\n\n2 4\n1\n0.000080\n\n\n1 2 3\n2\n0.000159\n\n\n2 3\n14\n0.001114\n\n\n1 2\n35\n0.002785\n\n\n1 3\n91\n0.007241\n\n\n2\n195\n0.015516\n\n\n3 4\n284\n0.022597\n\n\n4\n516\n0.041057\n\n\n1\n769\n0.061187\n\n\n3\n4759\n0.378660\n\n\n0\n5902\n0.469605\n\n\n\n\n\n\n\n\n\nMulti class defects\nThe get_classification_df allows to build a DataFrame to classification models. In ClassId_multi are listed the ClassIds separated by a space.\n\nsource\n\n\nget_classification_df\n\n get_classification_df (df:pandas.core.frame.DataFrame)\n\nGet the DataFrame for the multiclass classification model\n\ntrain_multi = get_classification_df(train)\ntrain_multi.head()\n\n\n\n\n\n\n\nClassId\nImageId\nClassId_multi\n\n\n\n\n0\n0002cc93b.jpg\n1\n\n\n1\n00031f466.jpg\n0\n\n\n2\n000418bfc.jpg\n0\n\n\n3\n000789191.jpg\n0\n\n\n4\n0007a71bf.jpg\n3\n\n\n\n\n\n\n\n\ncount_pct(train_multi, column='ClassId_multi').sort_values(\"freq\", ascending=False)\n\n\n\n\n\n\n\n\nnum\nfreq\n\n\nClassId_multi\n\n\n\n\n\n\n0\n5902\n0.469605\n\n\n3\n4759\n0.378660\n\n\n1\n769\n0.061187\n\n\n4\n516\n0.041057\n\n\n3 4\n284\n0.022597\n\n\n2\n195\n0.015516\n\n\n1 3\n91\n0.007241\n\n\n1 2\n35\n0.002785\n\n\n2 3\n14\n0.001114\n\n\n1 2 3\n2\n0.000159\n\n\n2 4\n1\n0.000080\n\n\n\n\n\n\n\n\ntrain_multi.describe(include='all')\n\n\n\n\n\n\n\nClassId\nImageId\nClassId_multi\n\n\n\n\ncount\n12568\n12568\n\n\nunique\n12568\n11\n\n\ntop\n0002cc93b.jpg\n0\n\n\nfreq\n1\n5902"
  },
  {
    "objectID": "eda.html#test-data",
    "href": "eda.html#test-data",
    "title": "Exploration Data Analysis",
    "section": "Test data",
    "text": "Test data\n\ntest_pfiles\n\n(#5506) [Path('../data/test_images/0000f269f.jpg'),Path('../data/test_images/000ccc2ac.jpg'),Path('../data/test_images/002451917.jpg'),Path('../data/test_images/003c5da97.jpg'),Path('../data/test_images/0042e163f.jpg'),Path('../data/test_images/004f40c73.jpg'),Path('../data/test_images/00513039a.jpg'),Path('../data/test_images/006f39c41.jpg'),Path('../data/test_images/008725cbc.jpg'),Path('../data/test_images/0098ca44e.jpg')...]\n\n\n\ntest_df = pd.read_csv(path / 'sample_submission.csv')\ntest_df.head()\n\n\n\n\n\n\n\n\nImageId\nEncodedPixels\nClassId\n\n\n\n\n0\n0000f269f.jpg\n1 409600\n0\n\n\n1\n000ccc2ac.jpg\n1 409600\n0\n\n\n2\n002451917.jpg\n1 409600\n0\n\n\n3\n003c5da97.jpg\n1 409600\n0\n\n\n4\n0042e163f.jpg\n1 409600\n0"
  },
  {
    "objectID": "eda.html#class-defects",
    "href": "eda.html#class-defects",
    "title": "Exploration Data Analysis",
    "section": "Class Defects",
    "text": "Class Defects\n\ndef old_rle2mask(rle: str, value: int, shape):\n    \"\"\"\n    From a RLE encoded pixels returns a mask\n    with `value` for defected pixels\n    (e.g. `value`=1 so 1 -&gt; defected, 0 -&gt; groundtruth)\n    and `shape` as tuple (height, width).\n    \"\"\"\n    assert len(shape) == 2, \"The shape must be (height, width)\"\n    assert isinstance(shape[0], int)\n    assert isinstance(shape[1], int)\n\n    h, w = shape\n    mask = np.zeros(h * w, dtype=np.uint8)\n\n    rle = rle.split(\" \")\n    positions = map(int, rle[0::2])\n    length = map(int, rle[1::2])\n\n    for pos, le in zip(positions, length):\n        mask[pos:(pos + le)] = value\n\n    return mask.reshape(h, w, order='F')\n\n\nsource\n\nrle2mask\n\n rle2mask (rle, value=1, shape=(256, 1600))\n\nmask_rle: run-length as string formated (start length) shape: (width,height) of array to return Returns numpy array, 1 - mask, 0 - background Source: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n\nitem = train_faulty.iloc[1]\nitem_class_id = item[\"ClassId\"]\nitem_rle = item[\"EncodedPixels\"]\nitem_rle[:50]\n\n'18661 28 18863 82 19091 110 19347 110 19603 110 19'\n\n\n\ns = item_rle.split()\nhow_many_values = 10\npixels = s[0:][::2]\nlengths = s[1:][::2]\n(pixels[:how_many_values], lengths[:how_many_values])\n\n(['18661',\n  '18863',\n  '19091',\n  '19347',\n  '19603',\n  '19859',\n  '20115',\n  '20371',\n  '20627',\n  '20883'],\n ['28', '82', '110', '110', '110', '110', '110', '110', '110', '110'])\n\n\n\nstarts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\nstarts -= 1\nstarts, lengths\n\n(array([ 18660,  18862,  19090,  19346,  19602,  19858,  20114,  20370,\n         20626,  20882,  21138,  21394,  21650,  21961, 293124, 293380,\n        293636, 293892, 294148, 294404, 294660, 294916, 295172, 295428,\n        295684, 295940, 296196, 296452, 296708, 296964, 297220, 297476,\n        297732, 297988, 298244, 298563, 298944]),\n array([ 28,  82, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n         55, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n        251, 251, 251, 251, 251, 251, 251, 251, 251, 188,  63]))\n\n\n\nends = starts + lengths\nends\n\narray([ 18688,  18944,  19200,  19456,  19712,  19968,  20224,  20480,\n        20736,  20992,  21248,  21504,  21760,  22016, 293375, 293631,\n       293887, 294143, 294399, 294655, 294911, 295167, 295423, 295679,\n       295935, 296191, 296447, 296703, 296959, 297215, 297471, 297727,\n       297983, 298239, 298495, 298751, 299007])\n\n\n\nmask_img = np.zeros(256*1600, dtype=np.uint8)\nfor lo, hi in zip(starts, ends):\n    mask_img[lo:hi] = 1\nmask_img[18658:18698]\n\narray([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)\n\n\n\nitem_mask = rle2mask(item_rle, 1)\nshow_image(item_mask, figsize=(15,5));\n\n\nsource\n\n\nmake_mask\n\n make_mask (item, df, flatten=False)\n\n*Given an item as: - row index [int] or - ImageId [str] or - file [Path] or - query [pd.Series],\nreturns the image_item and mask with two types of shapes: - (256, 1600) if flatten, - (256, 1600, 4) if not flatten,*\n\nsource\n\n\nmask2rle\n\n mask2rle (mask)\n\n*Efficient implementation of mask2rle, from @paulorzp\nimg: numpy array, 1 - mask, 0 - background Returns run length as string formated Source: https://www.kaggle.com/xhlulu/efficient-mask2rle*\n\nmask = rle2mask(item_rle)\nrle = mask2rle(mask)\nrle[:100]\n\n'18661 28 18863 82 19091 110 19347 110 19603 110 19859 110 20115 110 20371 110 20627 110 20883 110 21'\n\n\n\ntest_eq(rle, item_rle)\n\n\nsource\n\n\nplot_mask_image\n\n plot_mask_image (name:str, img:&lt;built-infunctionarray&gt;, mask:&lt;built-\n                  infunctionarray&gt;)\n\nPlot a np.array image and mask with contours.\n\nsource\n\n\nplot_defected_image\n\n plot_defected_image (img_path:pathlib.Path,\n                      df:pandas.core.frame.DataFrame, class_id=None)\n\nPlot a img_path Path image from the training folder with contours.\n\nsource\n\n\nget_random_idx\n\n get_random_idx (n:int)\n\nReturn a random sequence of size n."
  },
  {
    "objectID": "eda.html#plots",
    "href": "eda.html#plots",
    "title": "Exploration Data Analysis",
    "section": "Plots",
    "text": "Plots\n\nsource\n\nshow_defects\n\n show_defects (path, df, class_id=None, n=20, only_defects=True,\n               multi_defects=False)\n\nPlot multiple images. Attributes: path: [Path] df: [pd.DataFrame] only train_pivot class_id: [str or int] select a type of defect otherwise plot all kinds; n: select the number of images to plot; only_defects [bool, default True]: if False it shows even the no faulty images; multi_defects [bool, default False]: if True it shows imgs with multi defects.\n\nshow_defects(path, train_pivot)"
  },
  {
    "objectID": "loss.html",
    "href": "loss.html",
    "title": "Loss functions",
    "section": "",
    "text": "from fastai.vision.all import *\nimport numpy as np\nfrom torch.nn.modules.loss import _Loss\nimport segmentation_models_pytorch as smp\nfrom steel_segmentation.utils import get_train_df\nfrom steel_segmentation.transforms import SteelDataBlock, SteelDataLoaders\n\n\npath = Path(\"../data\")\ntrain_pivot = get_train_df(path=path, pivot=True)\nblock = SteelDataBlock(path)\ndls = SteelDataLoaders(block, train_pivot, bs=8)\nxb, yb = dls.one_batch()\nprint(xb.shape, xb.device)\nprint(yb.shape, yb.device)\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\n\n\ntorch.Size([8, 3, 224, 1568]) cuda:0\ntorch.Size([8, 4, 224, 1568]) cpu\n\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n'cuda'\n\n\n\nmodel = smp.Unet(\"resnet18\", classes=4).to(device)\n\nlogits = model(xb)\nprobs = torch.sigmoid(logits)\npreds = ( probs &gt; 0.5).float()\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\nsource\n\nSoftDiceLoss\n\n SoftDiceLoss ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ncriterion = SoftDiceLoss()\ncriterion(logits.detach().cpu(), yb)\n\nTensorImage(0.9883)\n\n\n\nsource\n\n\nWeightedSoftDiceLoss\n\n WeightedSoftDiceLoss (size_average=True, weight=[0.2, 0.8])\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ncriterion = WeightedSoftDiceLoss()\ncriterion(logits.detach().cpu(), yb)\n\nTensorMask(0.9471)\n\n\n\nsource\n\n\nSoftBCEDiceLoss\n\n SoftBCEDiceLoss (bce_pos_weight, size_average=True, dice_weights=[0.2,\n                  0.8], loss_weights=[0.7, 0.3])\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ncriterion = SoftBCEDiceLoss(bce_pos_weight=1.5)\ncriterion(logits.detach().cpu(), yb)\n\nTensorBase(0.7872)\n\n\n\nsource\n\n\nMultiClassesSoftBCEDiceLoss\n\n MultiClassesSoftBCEDiceLoss (classes_num=4, size_average=True,\n                              dice_weights=[0.2, 0.8],\n                              bce_pos_weights=[2.0, 2.0, 1.0, 1.5],\n                              loss_weights=[0.7, 0.3], thresh=0.5)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\ncriterion = MultiClassesSoftBCEDiceLoss()\nloss = criterion(logits.detach().cpu(), yb)\nloss\n\nTensorBase(0.7833)\n\n\n\ncriterion.decodes(logits.detach().cpu())\n\ntorch.Size([8, 224, 1568])\n\n\n\ncriterion.activation(logits.detach().cpu()).shape\n\ntorch.Size([8, 4, 224, 1568])\n\n\nFor the Tensorboard callback we need this Learner Callback to handle the step after the prediction.\n\nsource\n\n\nLossEnabler\n\n LossEnabler (after_create=None, before_fit=None, before_epoch=None,\n              before_train=None, before_batch=None, after_pred=None,\n              after_loss=None, before_backward=None,\n              after_cancel_backward=None, after_backward=None,\n              before_step=None, after_cancel_step=None, after_step=None,\n              after_cancel_batch=None, after_batch=None,\n              after_cancel_train=None, after_train=None,\n              before_validate=None, after_cancel_validate=None,\n              after_validate=None, after_cancel_epoch=None,\n              after_epoch=None, after_cancel_fit=None, after_fit=None)\n\nCast predictions and labels to TensorBase to compute the smp.losses\n\ndls.valid.bs\n\n8"
  },
  {
    "objectID": "segmentation_unet_resnet.html",
    "href": "segmentation_unet_resnet.html",
    "title": "Segmentation training Unet",
    "section": "",
    "text": "# all_slow"
  },
  {
    "objectID": "segmentation_unet_resnet.html#setup",
    "href": "segmentation_unet_resnet.html#setup",
    "title": "Segmentation training Unet",
    "section": "Setup",
    "text": "Setup\n\nfrom fastai.vision.all import *\nfrom fastai.callback.tensorboard import TensorBoardCallback\n\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport albumentations as alb\n\nimport segmentation_models_pytorch as smp\n\nfrom steel_segmentation.utils import get_train_df\nfrom steel_segmentation.transforms import SteelDataBlock, SteelDataLoaders\nfrom steel_segmentation.losses import MultiClassesSoftBCEDiceLoss, LossEnabler\nfrom steel_segmentation.metrics import ModDiceMulti\nfrom steel_segmentation.optimizer import opt_func\n\n\ndef seed_everything(seed=69):\n    \"\"\"\n    Seeds `random`, `os.environ[\"PYTHONHASHSEED\"]`,\n    `numpy`, `torch.cuda` and `torch.backends`.\n    \"\"\"\n    #warnings.filterwarnings(\"ignore\")\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()\n\nTraining parameters:\n\nbs = 16\nsize = (224,512)\nepochs = 30\nlr = 3e-4\npath = Path(\"../data\") # where data dir is"
  },
  {
    "objectID": "segmentation_unet_resnet.html#data-loading",
    "href": "segmentation_unet_resnet.html#data-loading",
    "title": "Segmentation training Unet",
    "section": "Data loading",
    "text": "Data loading\n\ndf = get_train_df(path, only_faulty=True, pivot=True)\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\nClassId\n1\n2\n3\n4\nn\nClassIds\n\n\n\n\ncount\n890\n245\n5078\n789\n6578.000000\n6578\n\n\nunique\n890\n245\n5078\n789\nNaN\n9\n\n\ntop\n29102 12 29346 24 29602 24 29858 24 30114 24 30370 24 30626 24 30882 24 31139 23 31395 23 31651 23 31907 23 32163 23 32419 23 32675 23 77918 27 78174 55 78429 60 78685 64 78941 68 79197 72 79452 77 79708 81 79964 85 80220 89 80475 94 80731 98 80987 102 81242 105 81498 105 81754 104 82010 104 82265 105 82521 31 82556 69 82779 27 82818 63 83038 22 83080 57 83297 17 83342 50 83555 13 83604 44 83814 8 83866 37 84073 3 84128 31 84390 25 84652 18 84918 8 85239 10 85476 29 85714 47 85960 57 86216 57 86471 58 86727 58 86983 58 87238 59 87494 59 87750 59 88005 60 88261 60 88517 60 88772 61 89028 53...\n145658 7 145901 20 146144 33 146386 47 146629 60 146872 73 147115 86 147364 93 147620 93 147876 93 148132 93 148388 93 148644 93 148900 93 149156 93 149412 93 149668 46\n18661 28 18863 82 19091 110 19347 110 19603 110 19859 110 20115 110 20371 110 20627 110 20883 110 21139 110 21395 110 21651 110 21962 55 293125 251 293381 251 293637 251 293893 251 294149 251 294405 251 294661 251 294917 251 295173 251 295429 251 295685 251 295941 251 296197 251 296453 251 296709 251 296965 251 297221 251 297477 251 297733 251 297989 251 298245 251 298564 188 298945 63\n131973 1 132228 4 132483 6 132738 8 132993 11 133248 13 133503 16 133757 19 134012 22 134267 24 134522 26 134777 29 135032 31 135287 34 135542 36 135796 40 136050 43 136304 46 136558 50 136812 54 137066 56 137320 59 137574 61 137828 63 138082 65 138336 68 138590 70 138845 71 139101 71 139356 73 139612 73 139868 73 140123 74 140379 74 140634 75 140890 75 141145 77 141400 78 141654 80 141909 81 142164 82 142418 84 142673 85 142928 86 143182 88 143437 89 143692 90 143946 93 144201 94 144456 95 144710 97 144965 98 145220 99 145474 101 145729 103 145983 105 146237 107 146491 109 146745 112 1469...\nNaN\n3\n\n\nfreq\n1\n1\n1\n1\nNaN\n4691\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n1.064457\nNaN\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n0.246820\nNaN\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n1.000000\nNaN\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n1.000000\nNaN\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n1.000000\nNaN\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n1.000000\nNaN\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n3.000000\nNaN\n\n\n\n\n\n\n\n\ndef get_train_aug(height, width): \n    tfm_list = [\n        alb.RandomCrop(height, width, p=1.0),\n        alb.OneOf(\n          [\n           alb.VerticalFlip(p=0.5),\n           alb.HorizontalFlip(p=0.5),\n          ], p=0.5),\n        alb.RandomBrightnessContrast(\n            brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n    ]\n    return alb.Compose(tfm_list)\n\ndef get_valid_aug(height, width): \n    tfms = [alb.RandomCrop(height, width, p=1.0)]\n    return alb.Compose(tfms)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrain_aug = get_train_aug(*size)\nvalid_aug = get_valid_aug(*size)\nblock = SteelDataBlock(path, train_aug=train_aug, valid_aug=valid_aug)\ndls = SteelDataLoaders(block, df, bs=bs, device=device)\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\nTo keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n  return torch.floor_divide(self, other)\n\n\n\nxb, yb = dls.one_batch()\nprint(xb.shape, yb.shape)\n\ntorch.Size([16, 3, 224, 512]) torch.Size([16, 4, 224, 512])"
  },
  {
    "objectID": "segmentation_unet_resnet.html#model",
    "href": "segmentation_unet_resnet.html#model",
    "title": "Segmentation training Unet",
    "section": "Model",
    "text": "Model\n\nmodel = smp.Unet(encoder_name=\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)\ncriterion = BCEWithLogitsLossFlat(pos_weight=torch.tensor([2.0,2.0,1.0,1.5])) # pos_weight because class imbalance\n#opt_func = partial(opt_func, torch_opt=torch.optim.Adam) # no need to use pytorch optim\nopt_func = RAdam\nmodel_dir = Path(\"../models\")\nmetrics = [ModDiceMulti(with_logits=True)]\n\n\nlearner = Learner(\n    dls = dls,\n    model = model,\n    loss_func = criterion,\n    opt_func = opt_func,\n    metrics = metrics,\n    model_dir = model_dir,\n    cbs = [LossEnabler]\n)\n\nC:\\Users\\beanTech\\miniconda3\\envs\\steel_segmentation\\lib\\site-packages\\fastai\\callback\\core.py:51: UserWarning: You are shadowing an attribute (__class__) that exists in the learner. Use `self.learn.__class__` to avoid this\n  warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n\n\n\nlearner.summary()\n\n\n\n\nc:\\users\\beantech\\documents\\python scripts\\steel_segmentation\\steel_segmentation\\metrics.py:56: RuntimeWarning: Mean of empty slice\n  return np.nanmean(binary_dice_scores)\n\n\nUnet (Input shape: 16)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     16 x 64 x 112 x 256 \nConv2d                                    9408       True      \nBatchNorm2d                               128        True      \nReLU                                                           \nMaxPool2d                                                      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \n____________________________________________________________________________\n                     16 x 128 x 28 x 64  \nConv2d                                    73728      True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nConv2d                                    8192       True      \nBatchNorm2d                               256        True      \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \n____________________________________________________________________________\n                     16 x 256 x 14 x 32  \nConv2d                                    294912     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nConv2d                                    32768      True      \nBatchNorm2d                               512        True      \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \n____________________________________________________________________________\n                     16 x 512 x 7 x 16   \nConv2d                                    1179648    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nConv2d                                    131072     True      \nBatchNorm2d                               1024       True      \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nReLU                                                           \nConv2d                                    2359296    True      \nBatchNorm2d                               1024       True      \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 256 x 14 x 32  \nConv2d                                    1769472    True      \nBatchNorm2d                               512        True      \nReLU                                                           \nIdentity                                                       \nConv2d                                    589824     True      \nBatchNorm2d                               512        True      \nReLU                                                           \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 128 x 28 x 64  \nConv2d                                    442368     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nIdentity                                                       \nConv2d                                    147456     True      \nBatchNorm2d                               256        True      \nReLU                                                           \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 64 x 56 x 128  \nConv2d                                    110592     True      \nBatchNorm2d                               128        True      \nReLU                                                           \nIdentity                                                       \nConv2d                                    36864      True      \nBatchNorm2d                               128        True      \nReLU                                                           \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 32 x 112 x 256 \nConv2d                                    36864      True      \nBatchNorm2d                               64         True      \nReLU                                                           \nIdentity                                                       \nConv2d                                    9216       True      \nBatchNorm2d                               64         True      \nReLU                                                           \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 16 x 224 x 512 \nConv2d                                    4608       True      \nBatchNorm2d                               32         True      \nReLU                                                           \nConv2d                                    2304       True      \nBatchNorm2d                               32         True      \nReLU                                                           \nIdentity                                                       \n____________________________________________________________________________\n                     16 x 4 x 224 x 512  \nConv2d                                    580        True      \nIdentity                                                       \nIdentity                                                       \n____________________________________________________________________________\n\nTotal params: 14,328,644\nTotal trainable params: 14,328,644\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function RAdam at 0x000001E9DC086DC0&gt;\nLoss function: FlattenedLoss of BCEWithLogitsLoss()\n\nCallbacks:\n  - TrainEvalCallback\n  - LossEnabler\n  - Recorder\n  - ProgressCallback\n\n\n\nlearner.show_training_loop()\n\nStart Fit\n   - before_fit     : [TrainEvalCallback, Recorder, ProgressCallback]\n  Start Epoch Loop\n     - before_epoch   : [Recorder, ProgressCallback]\n    Start Train\n       - before_train   : [TrainEvalCallback, Recorder, ProgressCallback]\n      Start Batch Loop\n         - before_batch   : []\n         - after_pred     : [LossEnabler]\n         - after_loss     : []\n         - before_backward: []\n         - before_step    : []\n         - after_step     : []\n         - after_cancel_batch: []\n         - after_batch    : [TrainEvalCallback, Recorder, ProgressCallback]\n      End Batch Loop\n    End Train\n     - after_cancel_train: [Recorder]\n     - after_train    : [Recorder, ProgressCallback]\n    Start Valid\n       - before_validate: [TrainEvalCallback, Recorder, ProgressCallback]\n      Start Batch Loop\n         - **CBs same as train batch**: []\n      End Batch Loop\n    End Valid\n     - after_cancel_validate: [Recorder]\n     - after_validate : [Recorder, ProgressCallback]\n  End Epoch Loop\n   - after_cancel_epoch: []\n   - after_epoch    : [Recorder]\nEnd Fit\n - after_cancel_fit: []\n - after_fit      : [ProgressCallback]\n\n\nLogging with the TensorBoardCallback:\n\n# logging info\nlog_dir = Path(\"../logs\") / f\"unet_resnet_bce_epochs{epochs}_lr{lr}\"\nlog_dir\n\nPath('../logs/unet_resnet_bce_epochs30_lr0.0003')\n\n\n\ntrain_cbs = [\n    TensorBoardCallback(log_dir=log_dir, log_preds=True, trace_model=True, projector=False),\n    GradientAccumulation(n_acc=24),\n    SaveModelCallback(monitor=\"valid_loss\", fname=log_dir.name, with_opt=True),\n]\n\n\nlearner.fit(epochs, lr=lr, cbs=train_cbs)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nmod_dice_multi\ntime\n\n\n\n\n0\n0.051801\n0.040153\n0.527716\n04:04"
  },
  {
    "objectID": "optimizer.html",
    "href": "optimizer.html",
    "title": "Optimizer",
    "section": "",
    "text": "Some utility functions for training Pytorch models with FastAI fine tuning method. The code is from this repository.\n\nsource\n\nparams\n\n params (m)\n\nReturn all parameters of m (Pytorch model).\n\nsource\n\n\nconvert_params\n\n convert_params (o:list)\n\n*Converts o into Pytorch-compatable param groups o should be a set of layer-groups that should be split in the optimizer Example:\ndef splitter(m): return convert_params([[m.a], [m.b]])\nWhere m is a model defined as: python class RegModel(Module):   def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))   def forward(self, x): return x*self.a + self.b*\nIn the FastAI library is largely used transfer learning with layer-group learning rate freezing. The convert_params function returns a list of parameters for specific layers in a model that allows discriminative learning rates.\n\nsource\n\n\nsmp_splitter\n\n smp_splitter (model)\n\n\nsource\n\n\nopt_func\n\n opt_func (params, torch_opt, *args, **kwargs)\n\nPytorch Optimizer for fastai Learner."
  }
]