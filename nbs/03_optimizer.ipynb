{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Utility functions for FastAI optimizers in Pytorch models.\n",
    "output-file: optimizer.html\n",
    "title: Optimizer\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp optimizer\n",
    "# all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions for training Pytorch models with FastAI fine tuning method. The code is from [this repository](https://github.com/muellerzr/fastai_minima/blob/master/fastai_minima/optimizer.py#L159)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "from fastcore.foundation import L\n",
    "from fastai.optimizer import OptimWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m` (Pytorch model).\"\n",
    "    return [p for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_params(o:list) -> list:\n",
    "    \"\"\"\n",
    "    Converts `o` into Pytorch-compatable param groups\n",
    "    `o` should be a set of layer-groups that should be split in the optimizer\n",
    "    Example:\n",
    "    ```python\n",
    "    def splitter(m): return convert_params([[m.a], [m.b]])\n",
    "    ```\n",
    "    Where `m` is a model defined as:\n",
    "    ```python\n",
    "    class RegModel(Module):\n",
    "      def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\n",
    "      def forward(self, x): return x*self.a + self.b\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if not isinstance(o[0], dict):\n",
    "        splitter = []\n",
    "        for group in o:\n",
    "            if not isinstance(group[0], nn.parameter.Parameter):\n",
    "                group = L(group).map(params)[0]\n",
    "            splitter.append({'params':group})\n",
    "        return splitter\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FastAI library is largely used transfer learning with layer-group learning rate freezing. \r\n",
    "The `convert_params` function returns a list of parameters for specific layers in a model that allows discriminative learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def smp_splitter(model):\n",
    "    return convert_params([[model.encoder],[model.decoder],[model.segmentation_head]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def opt_func(params, torch_opt, *args, **kwargs):\n",
    "    \"\"\"Pytorch Optimizer for fastai Learner.\"\"\"\n",
    "    return OptimWrapper(params, torch_opt, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28655ac308a3066e78dcd35793da650bd1be67b358d93812a1ea5ca876afb9ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('steel_segmentation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
