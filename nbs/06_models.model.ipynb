{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-nashville",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Deep Learning modules with Fastai/Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import graphviz\n",
    "def gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"' + s + '; }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from steel_segmentation.core import *\n",
    "from steel_segmentation.data import *\n",
    "from steel_segmentation.preprocessing import *\n",
    "from steel_segmentation.models.dls import *\n",
    "from steel_segmentation.models.metrics import *\n",
    "from steel_segmentation.models.unet import Unet\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    from fastai.vision.all import *\n",
    "    import fastai\n",
    "from fastcore.foundation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "only_imgs = [\"0a1cade03.jpg\", \"bca4ae758.jpg\", \"988cf521f.jpg\", \"b6a257b28.jpg\",\n",
    "             \"b2ad335bf.jpg\", \"72aaba8ad.jpg\", \"f383950e8.jpg\"]\n",
    "train = train[train[\"ImageId\"].isin(only_imgs)].copy()\n",
    "train_all = train_all[train_all[\"ImageId\"].isin(only_imgs)].copy()\n",
    "train_multi = train_multi[train_multi[\"ImageId\"].isin(only_imgs)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-defense",
   "metadata": {},
   "source": [
    "First we create a classification model to get an encoder that know how to classify defects pixels.\n",
    "Then, we build a UNet from the trained encoder and train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "models_dir = path.parent / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-residence",
   "metadata": {},
   "source": [
    "In the Paperspace Gradient machine I stored these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/ResNet18-Unet-256-stage1.pth\n",
      "../models/ResNet18-Unet-256-stage2.pth\n",
      "../models/ResNet18_kaggle_class.pth\n",
      "../models/ResNet18-Unet-256-stage3.pth\n",
      "../models/.ipynb_checkpoints\n",
      "../models/kaggle_model.pth\n",
      "../models/ResNet34-Unet-128-stage3.pth\n",
      "../models/ResNet34-Unet-128-stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# missing\n",
    "print_competition_data(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-catholic",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class_metrics = [accuracy_multi, PrecisionMulti(), RecallMulti()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-textbook",
   "metadata": {},
   "source": [
    "These lines of code allow me to create a classification `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "bs = 4\n",
    "\n",
    "dls = get_classification_dls(bs)\n",
    "arch = partial(resnet18, pretrained=True)\n",
    "class_learner = cnn_learner(\n",
    "    dls=dls, arch=arch, metrics=class_metrics, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-accident",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "seg_metrics = [DiceMulti(), dice_kaggle]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-agenda",
   "metadata": {},
   "source": [
    "These lines of code allow me to create a segmentation `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "bs = 4\n",
    "szs = (128, 800)\n",
    "\n",
    "dls = get_segmentation_dls_from_df(train_multi, bs, szs)\n",
    "segmentation_learner = unet_learner(\n",
    "    dls=dls, arch=resnet18, metrics=seg_metrics, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-decrease",
   "metadata": {},
   "source": [
    "To load a custom head in a Unet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "encoder_path = models_dir / \"ResNet18-2_class.pt\"\n",
    "segmentation_learner.model[0].load_state_dict(\n",
    "    torch.load(encoder_path), strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-grave",
   "metadata": {},
   "source": [
    "## Kaggle kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-values",
   "metadata": {},
   "source": [
    "The code in this project is not only FastAi. I based an alternative solution based on this [kernel](https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88). \n",
    "In this notebook I will go through each part of the model from that kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "seed = 69\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "model = Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Trainer:\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 16, \"val\": 8}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 20\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                phase=phase,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        \n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        \"\"\"\n",
    "        Forward pass: \n",
    "            load to GPU the imgs and masks,\n",
    "            calculate predictions,\n",
    "            calculate loss\n",
    "        \n",
    "        Returns:\n",
    "            loss and predictions\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        preds = self.net(images)\n",
    "        loss = self.loss_fn(preds, masks)\n",
    "        return loss, preds\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        \"\"\"\n",
    "        Iterate throught each batch in training or validatio phase.\n",
    "        \"\"\"\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ‚è∞: {start}\")\n",
    "        \n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader): # replace `dataloader` with `tk0` for tqdm\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        \n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Training loop for each epochs.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.iterate(epoch, \"val\")\n",
    "                self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./models/kaggle_model.pth\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Trainer.__init__\" class=\"doc_header\"><code>Trainer.__init__</code><a href=\"__main__.py#L5\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Trainer.__init__</code>(**`model`**)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Trainer.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Trainer.forward\" class=\"doc_header\"><code>Trainer.forward</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Trainer.forward</code>(**`images`**, **`targets`**)\n",
       "\n",
       "Forward pass: \n",
       "    load to GPU the imgs and masks,\n",
       "    calculate predictions,\n",
       "    calculate loss\n",
       "\n",
       "Returns:\n",
       "    loss and predictions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Trainer.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Trainer.iterate\" class=\"doc_header\"><code>Trainer.iterate</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Trainer.iterate</code>(**`epoch`**, **`phase`**)\n",
       "\n",
       "Iterate throught each batch in training or validatio phase."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Trainer.iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-fifteen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Trainer.start\" class=\"doc_header\"><code>Trainer.start</code><a href=\"__main__.py#L95\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Trainer.start</code>()\n",
       "\n",
       "Training loop for each epochs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Trainer.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_models.dls.ipynb.\n",
      "Converted 04_model.metrics.ipynb.\n",
      "Converted 05_models.module.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
